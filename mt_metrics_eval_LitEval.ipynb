{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGuFoP_Gq9X9"
      },
      "source": [
        "This notebook is modified based on the original mt_metrics_eval notebook. \n",
        "\n",
        "In order to run the script, please modify the meta_info.py by adding the following meta to include the dataset:\n",
        "\n",
        "'''\n",
        "LITEVAL = MetaInfo('src', {'sys': 'mqm_score', 'seg': 'mqm_score'}, set(), set())\n",
        "\n",
        "LITEVAL_PRIMARIES = {'mqm_score', 'rating', 'cometkiwi', 'comet_xl', 'comet_xxl', 'm-pro', 'treqa',\n",
        "       'openai_gpt-4o-mini_final_set_with_Qlevel_step',\n",
        "       'openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted',\n",
        "       'meta-llama_llama-3.3-70b-instruct_PAR3-final_set_with_plevel_stepv2',\n",
        "       'meta-llama_llama-3.3-70b-instruct_PAR3-final_set_with_plevel_stepv2_weighted',\n",
        "       'openai_gpt-4o-mini_final_set_with_plevel_stepv2',\n",
        "       'openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted',\n",
        "       'openai_gpt-4o-mini_PAR3-final_set_with_plevel_stepv2',\n",
        "       'openai_gpt-4o-mini_PAR3-final_set_with_plevel_stepv2_weighted',\n",
        "       'meta-llama_llama-3.3-70b-instruct_PAR3-final_set_with_QA',\n",
        "       'meta-llama_llama-3.3-70b-instruct_PAR3-final_set_with_QA_weighted',\n",
        "       'openai_gpt-4o-mini_final_set_with_QA',\n",
        "       'openai_gpt-4o-mini_final_set_with_QA_weighted',\n",
        "       'Qwen_Qwen2.5-32B-Instruct_final_set_with_plevel_stepv2',\n",
        "       'Qwen_Qwen2.5-32B-Instruct_final_set_with_plevel_stepv2_weighted',\n",
        "       'meta-llama_llama-3.1-405b-instruct_final_set_with_plevel_stepv2',\n",
        "       'meta-llama_llama-3.1-405b-instruct_final_set_with_plevel_stepv2_weighted',\n",
        "       'openai_gpt-4o-mini_PAR3-final_set_with_QA',\n",
        "       'openai_gpt-4o-mini_PAR3-final_set_with_QA_weighted',\n",
        "       'qwen_qwen-2.5-72b-instruct_final_set_with_plevel_stepv2',\n",
        "       'qwen_qwen-2.5-72b-instruct_final_set_with_plevel_stepv2_weighted',\n",
        "       'meta-llama_llama-3.3-70b-instruct_final_set_with_plevel_stepv2',\n",
        "       'meta-llama_llama-3.3-70b-instruct_final_set_with_plevel_stepv2_weighted',\n",
        "       'quar_full_20250320_2116568550', 'quar_fr_en_20250320_19440812300',\n",
        "       'half_xxen_20250320_16390712500', 'quar_xxen_20250320_21051212150',\n",
        "       'half_full_20250321_11562622350', 'half_fr_en_20250320_16162112500',\n",
        "}\n",
        "\n",
        "LITEVAL_BASELINES = LITEVAL_PRIMARIES\n",
        "'''\n",
        "\n",
        "\n",
        "\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
        "This is a demo colab for MTME. It assumes you have mt_metrics_eval installed on your runtime, and have downloaded the data onto that machine. Run the cells below in order.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH8o8UKmUhQ8"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "folder = \"datasets/\"\n",
        "df_list = pd.read_csv(os.path.join(folder, \"LitEval_human_annotated_init_bench.csv\")) # score from previous study\n",
        "df_list[\"source_\"] = df_list[\"source\"].apply(lambda x: x.replace(\"\\n \", \"\").replace(\"\\n\", \"\")[:50])\n",
        "df_list['source'] = df_list.source.apply(lambda x: x.replace(\"\\n\", \"  \")).fillna(\"\") # remove all spaces in source\n",
        "df_list['tgt'] = df_list.tgt.apply(lambda x: x.replace(\"\\n\", \"  \")).fillna(\"\") # remove all spaces in sou\n",
        "df_list['mqm_score'] = df_list.mqm_score.astype(float)\n",
        "df_list['rating'] = df_list.rating.astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load treqa\n",
        "import json\n",
        "files = [\"bench-lit2-1000.jsonl\", \"bench-lit2-2000.jsonl\", \"bench-lit2-3666.jsonl\",]\n",
        "res = []\n",
        "for file in files:\n",
        "    with open(\"eval_results/treqa/\" + file, \"r\") as f:\n",
        "        for line in f:\n",
        "            d = json.loads(line)\n",
        "            res.append(d)\n",
        "res = pd.DataFrame(res)\n",
        "res[\"treqa\"] = res[\"per_q_scores\"].apply(lambda x: np.mean(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load m-pro\n",
        "mp = pd.read_csv(\"eval_results/mpro/m-premetheous.csv\")\n",
        "def extract_number(text):\n",
        "    match = re.search(r\"\\[RESULT\\]\\s*(\\d+)\", text)\n",
        "    if match:\n",
        "        number = int(match.group(1))\n",
        "        return number\n",
        "    elif text[0].isdigit():\n",
        "        return int(text[0])\n",
        "    else:\n",
        "        # extract number from text\n",
        "        match = re.search(r\"(\\d+)\", text)\n",
        "        if match:\n",
        "            number = int(match.group(1))\n",
        "            return number\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "mp[\"score\"] = mp[\"llm_response\"].apply(extract_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/2n/4742tn7s13l5fcnstlm_d7g40000gn/T/ipykernel_26970/3262491330.py:9: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  del_set.groupby(\"dataset\")[\"pair\", \"ID\"].nunique()\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair</th>\n",
              "      <th>ID</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>LitEval</th>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>literarytran</th>\n",
              "      <td>18</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              pair  ID\n",
              "dataset               \n",
              "LitEval          4  70\n",
              "literarytran    18  99"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#subset only the test data: exclude del set\n",
        "del_set = pd.read_csv(\"datasets/sampled_benchmark.csv\")\n",
        "del_set_liteval = del_set[del_set[\"dataset\"] == \"LitEval\"]\n",
        "#n = 0\n",
        "#for src in del_set_liteval[\"src\"].unique():\n",
        "#    if not src[:50] in df_list[\"source\"].values:\n",
        "#       n = n+1\n",
        "del_set.groupby([\"dataset\"]).size()\n",
        "del_set.groupby(\"dataset\")[\"pair\", \"ID\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load new data\n",
        "df = pd.read_csv(\"datasets/benchmark_dataset_all_src_tgt.csv\")\n",
        "df_comet = pd.read_csv(\"eval_results/xcomet/comet_df_full.csv\")\n",
        "assert all(df_comet[\"ID\"] == df[\"ID\"])\n",
        "df[\"cometkiwi\"] = df_comet[\"cometkiwi\"]\n",
        "df[\"comet_xl\"] = df_comet[\"comet_xl\"]\n",
        "df[\"comet_xxl\"] = df_comet[\"comet_xxl\"]\n",
        "df[\"m-pro\"] = mp[\"score\"]\n",
        "df[\"treqa\"] = res[\"treqa\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['pair', 'model', 'ID', 'dataset', 'cometkiwi', 'comet_xl', 'comet_xxl',\n",
              "       'm-pro', 'treqa'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "def parse_answers(completion):\n",
        "    \"\"\"Extract YES/NO/MAYBE counts from a JSON response.\n",
        "    Handles both single and double quotes in the JSON string.\n",
        "    Removes markdown code block markers if present.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Remove markdown code block markers if present\n",
        "        completion = completion.strip().replace(\"```\", \"\").replace(\"json\\n\", \"\").replace('\"\"', '\"').replace(\"\\\\n\", \"\\n\").replace(\" \\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\") # Remove ```json\n",
        "        completion = completion.strip()\n",
        "        try:\n",
        "            completion = \"{\" + completion.split(\"}\\n{\")[1]\n",
        "        except:\n",
        "            completion = \"{\" + completion.split(\"{\")[1]\n",
        "        if \"}\" not in completion:\n",
        "            completion = completion + \"}\"\n",
        "        completion = completion.replace(\"'\", '\"')\n",
        "        answers = json.loads(completion)\n",
        "            # Count YES/NO/MAYBE\n",
        "        counts = {\n",
        "            \"YES\": sum(1 for v in answers.values() if v == \"YES\"),\n",
        "            \"NO\": sum(1 for v in answers.values() if v == \"NO\"),\n",
        "            \"MAYBE\": sum(1 for v in answers.values() if v == \"MAYBE\")\n",
        "        }\n",
        "        score = counts[\"YES\"] + 0.5*counts[\"MAYBE\"]-counts[\"NO\"]\n",
        "        return counts, answers, score\n",
        "    except:\n",
        "        completion = completion.replace('\"', \"'\")  \n",
        "        matches = re.findall(r\"'(\\d+)':\\s*'(YES|NO|MAYBE)'\", completion)\n",
        "        answers = {k: v for k, v in matches}\n",
        "        counts = {\n",
        "            \"YES\": sum(1 for v in answers.values() if v == \"YES\"),\n",
        "            \"NO\": sum(1 for v in answers.values() if v == \"NO\"),\n",
        "            \"MAYBE\": sum(1 for v in answers.values() if v == \"MAYBE\")\n",
        "        }\n",
        "        score = counts[\"YES\"] + 0.5*counts[\"MAYBE\"]-counts[\"NO\"]\n",
        "        return counts, answers, score\n",
        "    finally:\n",
        "        print(\"Error parsing answers:\", repr(completion))\n",
        "\n",
        "def parse_answers_simple(completion):\n",
        "    completion = completion.replace('\"', \"'\")  \n",
        "    matches = re.findall(r\"'(\\d+)':\\s*'(YES|NO|MAYBE)'\", completion)\n",
        "    answers = {k: v for k, v in matches}\n",
        "    counts = {\n",
        "            \"YES\": sum(1 for v in answers.values() if v == \"YES\"),\n",
        "            \"NO\": sum(1 for v in answers.values() if v == \"NO\"),\n",
        "            \"MAYBE\": sum(1 for v in answers.values() if v == \"MAYBE\")\n",
        "        }\n",
        "    score = counts[\"YES\"] + 0.5*counts[\"MAYBE\"]-counts[\"NO\"]\n",
        "    return counts, answers, score\n",
        "\n",
        "def score_function(df_list):\n",
        "    res = []\n",
        "    for r in df_list[\"response\"]:\n",
        "        res.append(parse_answers_simple(r))\n",
        "    df_list[\"score\"] = [i[2] if i is not None else None for i in res ]\n",
        "    df_list[\"answers\"] = [i[1] if i is not None else None for i in res]\n",
        "    df_list[\"counts\"] = [i[0] if i is not None else None for i in res]\n",
        "    \n",
        "    return df_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load prompt results\n",
        "folder = \"eval_results/litransproqa/\"\n",
        "for file in os.listdir(folder):\n",
        "    if file.endswith(\".csv\"):\n",
        "        col = file.replace(\".csv\", \"\")\n",
        "        weights = pd.read_csv(\"datasets/QA_weights.csv\").score.values\n",
        "        tmp = pd.read_csv(os.path.join(folder, file))\n",
        "        tmp = score_function(tmp)\n",
        "        df[col] = tmp[\"score\"]/5\n",
        "        df_ = pd.DataFrame(tmp[\"answers\"].apply(lambda x: list(x.values())).tolist())\n",
        "        df[col + \"_weighted\"] = df_.apply(lambda x: x.map({\"YES\": 1, \"NO\": 0, \"MAYBE\": 0.5})).mul(weights).mean(axis=1)\n",
        "df_ = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "folder = \"eval_results/xcomet_ranking/\"\n",
        "for file in os.listdir(folder):\n",
        "    if file.endswith(\".json\"):\n",
        "        with open(os.path.join(folder, file), \"r\") as f:\n",
        "            tmp = json.load(f)\n",
        "            col = file.replace(\".json\", \"\").replace(\"xcomet_ranking_\", \"\").replace(\"_checkpoints_checkpoint-\", \"\").replace(\"xcomet_ranking_\", \"\").replace(\"referencefree_output_\", \"\")\n",
        "            df[col] = tmp[\"segment_scores\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_list = pd.concat([df_list, df[df[\"dataset\"] == \"LitEval\"].iloc[:, 4:].reset_index(drop=True)], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#subset only the test data: exclude del set\n",
        "del_set = pd.read_csv(\"datasets/sampled_benchmark.csv\")\n",
        "del_set_liteval = del_set[del_set[\"dataset\"] == \"LitEval\"]\n",
        "df_list[\"ID\"] = df_list.apply(lambda x: x[\"source\"][:15].replace(\" \",\"\") + \"-\" + x[\"pair\"] + \"-\" + x[\"model_x\"], axis = 1)\n",
        "subshape = df_list.set_index(\"ID\").loc[del_set_liteval[\"ID\"].unique()].shape[0]\n",
        "assert subshape == del_set_liteval.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1976, 42)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask = [True if not id in del_set_liteval[\"ID\"].values else False for id in df_list[\"ID\"]]\n",
        "df_list = df_list[mask]\n",
        "df_list.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['source', 'tgt', 'model', 'mqm_score', 'rating', 'pair', 'model_x',\n",
              "       'source_', 'cometkiwi', 'comet_xl', 'comet_xxl', 'm-pro', 'treqa',\n",
              "       'openai_gpt-4o-mini_final_set_with_Qlevel_step',\n",
              "       'openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted',\n",
              "       'meta-llama_llama-3.3-70b-instruct_PAR3-final_set_with_plevel_stepv2',\n",
              "       'meta-llama_llama-3.3-70b-instruct_PAR3-final_set_with_plevel_stepv2_weighted',\n",
              "       'openai_gpt-4o-mini_final_set_with_plevel_stepv2',\n",
              "       'openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted',\n",
              "       'openai_gpt-4o-mini_PAR3-final_set_with_plevel_stepv2',\n",
              "       'openai_gpt-4o-mini_PAR3-final_set_with_plevel_stepv2_weighted',\n",
              "       'meta-llama_llama-3.3-70b-instruct_PAR3-final_set_with_QA',\n",
              "       'meta-llama_llama-3.3-70b-instruct_PAR3-final_set_with_QA_weighted',\n",
              "       'openai_gpt-4o-mini_final_set_with_QA',\n",
              "       'openai_gpt-4o-mini_final_set_with_QA_weighted',\n",
              "       'Qwen_Qwen2.5-32B-Instruct_final_set_with_plevel_stepv2',\n",
              "       'Qwen_Qwen2.5-32B-Instruct_final_set_with_plevel_stepv2_weighted',\n",
              "       'meta-llama_llama-3.1-405b-instruct_final_set_with_plevel_stepv2',\n",
              "       'meta-llama_llama-3.1-405b-instruct_final_set_with_plevel_stepv2_weighted',\n",
              "       'openai_gpt-4o-mini_PAR3-final_set_with_QA',\n",
              "       'openai_gpt-4o-mini_PAR3-final_set_with_QA_weighted',\n",
              "       'qwen_qwen-2.5-72b-instruct_final_set_with_plevel_stepv2',\n",
              "       'qwen_qwen-2.5-72b-instruct_final_set_with_plevel_stepv2_weighted',\n",
              "       'meta-llama_llama-3.3-70b-instruct_final_set_with_plevel_stepv2',\n",
              "       'meta-llama_llama-3.3-70b-instruct_final_set_with_plevel_stepv2_weighted',\n",
              "       'quar_full_20250320_2116568550', 'quar_fr_en_20250320_19440812300',\n",
              "       'half_xxen_20250320_16390712500', 'quar_xxen_20250320_21051212150',\n",
              "       'half_full_20250321_11562622350', 'half_fr_en_20250320_16162112500',\n",
              "       'ID'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_list.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = [\"mqm_score\",  'gemba_all_orig', 'comet_xl', 'comet_xxl', 'm-pro', \"cometkiwi\", 'treqa',\n",
        "           'openai_gpt-4o-mini_final_set_with_QA', # vanilla\n",
        "       'openai_gpt-4o-mini_final_set_with_QA_weighted',\n",
        "       'openai_gpt-4o-mini_final_set_with_plevel_stepv2', # promptstep\n",
        "       'openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted',\n",
        "       'openai_gpt-4o-mini_final_set_with_Qlevel_step', # questionstep\n",
        "       'openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted',    \n",
        "       'quar_fr_en_20250320_19440812300', # quater xcomet Fr-en\n",
        "       'quar_xxen_20250320_21051212150', # multi xcomet xx-en\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#build mt_metrics_eval dataset format\n",
        "ref = \"src\"\n",
        "df_format = df_list.pivot(index=[\"pair\", \"source_\"], columns=[\"model\"], values=\"mqm_score\").reset_index()\n",
        "df_format = df_format.merge(df_list[[\"pair\", \"source_\", \"source\"]], on=[\"pair\", \"source_\"], how=\"left\").drop_duplicates(subset=[\"pair\", \"source_\"], keep=\"first\").reset_index(drop=True)\n",
        "lst_src = df_format[\"source\"].values\n",
        "lst_src_ = df_format[\"source_\"].values\n",
        "lst_pair = df_format[\"pair\"].values\n",
        "lst_model = df_format.columns[2:-1]\n",
        "\n",
        "for p, g in df_format.groupby([\"pair\"]):\n",
        "    os.makedirs(f\"benchmark_dataset/liteval/sources/\", exist_ok=True)\n",
        "    os.makedirs(f\"benchmark_dataset/liteval/references/\", exist_ok=True)\n",
        "    g[[\"source\"]].fillna(\"NaN\").to_csv(f\"benchmark_dataset/liteval/sources/{p}.txt\", sep=\" \", index=False, header=False)\n",
        "    g[[\"source\"]].fillna(\"NaN\").to_csv(f\"benchmark_dataset/liteval/references/{p}-{ref}.txt\", sep=\" \", index=False, header=False)\n",
        "\n",
        "    os.makedirs(f\"benchmark_dataset/liteval/documents/\", exist_ok=True)\n",
        "    pd.DataFrame().to_csv(f\"benchmark_dataset/liteval/documents/{p}.docs\", sep=\" \", index=False, header=False)\n",
        "\n",
        "\n",
        "df_format = df_list.pivot(index=[\"pair\", \"source_\"], columns=[\"model\"], values=\"tgt\").reset_index()\n",
        "df_format = df_format.set_index([\"source_\", \"pair\"]).loc[(lst_src_, lst_pair), lst_model]\n",
        "\n",
        "for p, g in df_format.groupby([\"pair\"]):\n",
        "    for model in set(lst_model):\n",
        "        os.makedirs(f\"benchmark_dataset/liteval/system-outputs/{p}/\", exist_ok=True)\n",
        "        g[[model]].fillna(\"NaN\").to_csv(f\"benchmark_dataset/liteval/system-outputs/{p}/{model}.txt\", index=False, sep=\" \", header=False)\n",
        "\n",
        "\n",
        "for score in metrics:\n",
        "    df_format = df_list.pivot(index=[\"pair\", \"source_\"], columns=[\"model\"], values=score).reset_index()\n",
        "    df_format = df_format.set_index([\"source_\", \"pair\"]).loc[(lst_src_, lst_pair), lst_model]\n",
        "    print(score)\n",
        "    for p, g in df_format.groupby([\"pair\"]):\n",
        "        print(p)\n",
        "        os.makedirs(f\"benchmark_dataset/liteval/metric-scores/{p}/\", exist_ok=True)\n",
        "        os.makedirs(f\"benchmark_dataset/liteval/human-scores/\", exist_ok=True)\n",
        "        output_concat = pd.DataFrame()\n",
        "        for model in set(lst_model):\n",
        "            tmp = g[[model]]\n",
        "            tmp[\"model\"] = model\n",
        "            tmp.columns = [\"score\", \"model\"]\n",
        "            output_concat = pd.concat([output_concat, tmp])\n",
        "        if not score in [\"mqm_score\", \"rating\"]:\n",
        "            output_concat[[\"model\", \"score\"]].fillna(\"NaN\").to_csv(f\"benchmark_dataset/liteval/metric-scores/{p}/{score}-{ref}.seg.score\", index=False, sep=\" \", header=False)\n",
        "            output_concat.groupby(\"model\", as_index=False)[\"score\"].mean()[[\"model\", \"score\"]].to_csv(f\"benchmark_dataset/liteval/metric-scores/{p}/{score}-{ref}.sys.score\", index=False, sep=\" \", header=False)\n",
        "        else:\n",
        "            output_concat[[\"model\", \"score\"]].fillna(\"NaN\").to_csv(f\"benchmark_dataset/liteval/human-scores/{p}.{score}.seg.score\", index=False, sep=\" \", header=False)\n",
        "            output_concat.groupby(\"model\", as_index=False)[\"score\"].mean()[[\"model\", \"score\"]].to_csv(f\"benchmark_dataset/liteval/human-scores/{p}.{score}.sys.score\", index=False, sep=\" \", header=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "code",
        "id": "Cr0TM9EY7wOH"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "from pathlib import Path\n",
        "import sys\n",
        "# Setup COMET path\n",
        "COMET_ROOT = Path(\"mt_metrics_eval\")\n",
        "sys.path.append(str(COMET_ROOT))\n",
        "from mt_metrics_eval import meta_info\n",
        "from mt_metrics_eval import data\n",
        "from mt_metrics_eval import stats\n",
        "from mt_metrics_eval import tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "code",
        "id": "GznnWylA8gwJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wmt24pp: en-ar_EG en-ar_SA en-bg_BG en-bn_IN en-ca_ES en-cs_CZ en-da_DK en-de_DE en-el_GR en-es_MX en-et_EE en-fa_IR en-fi_FI en-fil_PH en-fr_CA en-fr_FR en-gu_IN en-he_IL en-hi_IN en-hr_HR en-hu_HU en-id_ID en-is_IS en-it_IT en-ja_JP en-kn_IN en-ko_KR en-lt_LT en-lv_LV en-ml_IN en-mr_IN en-nl_NL en-no_NO en-pa_IN en-pl_PL en-pt_BR en-pt_PT en-ro_RO en-ru_RU en-sk_SK en-sl_SI en-sr_RS en-sv_SE en-sw_KE en-sw_TZ en-ta_IN en-te_IN en-th_TH en-tr_TR en-uk_UA en-ur_PK en-vi_VN en-zh_CN en-zh_TW en-zu_ZA\n",
            "wmt24: en-de en-es ja-zh cs-uk en-cs en-hi en-is en-ja en-ru en-uk en-zh\n",
            "wmt23.sent: en-de\n",
            "wmt23: en-de he-en zh-en cs-uk de-en en-cs en-he en-ja en-ru en-uk en-zh ja-en ru-en uk-en\n",
            "wmt22: en-de en-ru zh-en cs-en cs-uk de-en de-fr en-cs en-hr en-ja en-liv en-uk en-zh fr-de ja-en liv-en ru-en ru-sah sah-ru uk-cs uk-en\n",
            "wmt21.news: en-cs en-de en-ha en-is en-ja en-ru en-zh cs-en de-en de-fr fr-de ha-en is-en ja-en ru-en zh-en\n",
            "wmt21.tedtalks: en-de en-ru zh-en\n",
            "wmt21.flores: bn-hi hi-bn xh-zu zu-xh\n",
            "wmt20: cs-en de-en en-cs en-de en-iu en-ja en-pl en-ru en-ta en-zh iu-en ja-en km-en pl-en ps-en ru-en ta-en zh-en\n",
            "wmt19: de-cs de-en de-fr en-cs en-de en-fi en-gu en-kk en-lt en-ru en-zh fi-en fr-de gu-en kk-en lt-en ru-en zh-en\n",
            "liteval: de-en en-de en-zh de-zh\n"
          ]
        }
      ],
      "source": [
        "# @title Print all available evalsets\n",
        "\n",
        "for testset in meta_info.DATA: # dictionary\n",
        "  print(f'{testset}:', ' '.join(lp for lp in meta_info.DATA[testset]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OfiSe4Yt8sz3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "liteval/de-en\n",
            "liteval/en-de\n",
            "liteval/en-zh\n",
            "liteval/de-zh\n",
            "name                  segs sys metrics gold  refs std\n",
            "liteval/de-en           45  12      14 mqm_score    0 src\n",
            "liteval/en-de           46  12      14 mqm_score    0 src\n",
            "liteval/en-zh           48  12      14 mqm_score    0 src\n",
            "liteval/de-zh           44  12      14 mqm_score    0 src\n"
          ]
        }
      ],
      "source": [
        "# @title Load data for WMT21 language pairs scored with MQM\n",
        "\n",
        "all_evs = {}  # name/lp -> evs\n",
        "for testset in meta_info.DATA:\n",
        "  if not testset.startswith('liteval'): continue\n",
        "  for lp in meta_info.DATA[testset]:\n",
        "    if 'mqm_score' in meta_info.DATA[testset][lp].std_gold.values():\n",
        "      all_evs[f'{testset}/{lp}'] = data.EvalSet(testset, lp, True, path=\"./benchmark_dataset\")\n",
        "\n",
        "print('\\n'.join(all_evs.keys()))\n",
        "\n",
        "# @title Print summaries for all loaded evalsets\n",
        "\n",
        "print(f'{\"name\":<20}  segs sys metrics gold  refs std')\n",
        "for name, evs in all_evs.items():\n",
        "  nsegs = len(evs.src)\n",
        "  nsys = len(evs.sys_names)\n",
        "  nmetrics = len(evs.metric_basenames)\n",
        "  gold = evs.StdHumanScoreName('sys')\n",
        "  nrefs = len(evs.ref_names)\n",
        "  std_ref = evs.std_ref\n",
        "\n",
        "  print(f'{name:<20} {nsegs:5d} {nsys:3d} {nmetrics:7d} '\n",
        "        f'{gold:5} {nrefs:4d} {std_ref}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW9Mda-jUpqh"
      },
      "source": [
        "# Comparing metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GSwjndy3mQeo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'src'}, {'src'}, {'src'}, {'src'}]\n"
          ]
        }
      ],
      "source": [
        "# @title Set up for comparing metrics\n",
        "\n",
        "# There are many different ways to evaluate the performance of MT metrics. The\n",
        "# most obvious question is what correlation statistic we should use to capture\n",
        "# the similarity between a vector of metric scores and a vector of gold scores\n",
        "# (human ratings). A less obvious question is where those vectors come from.\n",
        "# We'll defer the choice of correlation statistic to later cells, and begin\n",
        "# by setting some parameters that precisely define the vectors we're interested\n",
        "# in comparing.\n",
        "\n",
        "# Use all evalsets that we've loaded.\n",
        "evs_list = all_evs.values()\n",
        "\n",
        "# Choose the version of each metric that uses the standard reference for each\n",
        "# evalset.\n",
        "main_refs = [{evs.std_ref} for evs in evs_list]\n",
        "print(main_refs)\n",
        "\n",
        "\n",
        "# Include 'human' systems (ie, reference translations) among systems to be\n",
        "# scored. This can make the task more challenging, since some metrics are\n",
        "# biased against less literal references.\n",
        "include_human = True\n",
        "\n",
        "# Don't include systems considered to be outliers. These are systems that are\n",
        "# much better or worse than all other systems, so they are easy for all metrics\n",
        "# to rank correctly).\n",
        "include_outliers = False\n",
        "\n",
        "# Use MQM ratings as gold scores rather than the scores provided by the main\n",
        "# WMT task. Metrics tasks have used MQM for main results since 2021.\n",
        "gold_name = 'mqm_score'\n",
        "\n",
        "# Only compare metrics that have been designated as primary submissions. This\n",
        "# removes metric variants that are similar to each other, and reduces the size\n",
        "# of the comparison matrix.\n",
        "primary_metrics = True\n",
        "\n",
        "# Don't limit the results to a particular domain. In WMT21, domains are treated\n",
        "# as separate test-sets, so this is a no-op (WMT22 is a different story).\n",
        "domain = None\n",
        "\n",
        "# Set the number of resampling runs for determining whether one metric is better\n",
        "# than another according to the permutation test. We'll use 5 to make the demo\n",
        "# finish quickly, but at least 1000 is required for stable results.\n",
        "k = 1000\n",
        "\n",
        "# Set the size of blocks for 'early stopping' checks during resampling. If\n",
        "# you're using k = 1000, this can speed up the computation, usually with\n",
        "# only minimal changes to the results.\n",
        "psd = stats.PermutationSigDiffParams(block_size = 100)\n",
        "\n",
        "# Set the p-value for deciding wheter metrics are considered to be significantly\n",
        "# different. Lower values make the test more stringent.\n",
        "pval = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ur6XTs9hlmG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "en-de\n",
            "*openai_gpt-4o-mini_final_set_with_QA_weighted[noref]             1  0.6994126  . > > > > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_QA[noref]                      2  0.6944583  . . = = > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted[noref]    2  0.6933177  . . . = > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted[noref]  2  0.6895099  . . . . > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step[noref]             3  0.6888958  . . . . . = > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2[noref]           3  0.6881700  . . . . . . > > > > > > > >\n",
            "*m-pro[noref]                                                     4  0.6673118  . . . . . . . = = = = = = >\n",
            "*gemba_all_orig[noref]                                            4  0.6392037  . . . . . . . . = = = = = >\n",
            "*comet_xxl[noref]                                                 4  0.5992598  . . . . . . . . . > > > > >\n",
            "*cometkiwi[noref]                                                 5  0.5614781  . . . . . . . . . . > > > >\n",
            "*comet_xl[noref]                                                  6  0.5416486  . . . . . . . . . . . > > >\n",
            "*quar_fr_en_20250320_19440812300[noref]                           7  0.5303722  . . . . . . . . . . . . > >\n",
            "*quar_xxen_20250320_21051212150[noref]                            8  0.5028357  . . . . . . . . . . . . . >\n",
            "*treqa[noref]                                                     9  0.3110451  . . . . . . . . . . . . . . \n",
            "None\n",
            "de-en\n",
            "*openai_gpt-4o-mini_final_set_with_QA_weighted[noref]             1  0.5074059  . > > > > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_QA[noref]                      2  0.5050244  . . = > = > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted[noref]    2  0.4878785  . . . > > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step[noref]             3  0.4870147  . . . . = > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted[noref]  3  0.4797998  . . . . . > > > > > > > > >\n",
            "*gemba_all_orig[noref]                                            4  0.4787627  . . . . . . = > = = = > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2[noref]           4  0.4716641  . . . . . . . > > > > > > >\n",
            "*m-pro[noref]                                                     5  0.4298088  . . . . . . . . = = = = = =\n",
            "*treqa[noref]                                                     5  0.3418549  . . . . . . . . . > > > > >\n",
            "*comet_xxl[noref]                                                 6  0.3332531  . . . . . . . . . . > > > >\n",
            "*cometkiwi[noref]                                                 7  0.3268330  . . . . . . . . . . . > > >\n",
            "*comet_xl[noref]                                                  8  0.3063917  . . . . . . . . . . . . > >\n",
            "*quar_fr_en_20250320_19440812300[noref]                           9  0.2853388  . . . . . . . . . . . . . >\n",
            "*quar_xxen_20250320_21051212150[noref]                           10  0.2788493  . . . . . . . . . . . . . . \n",
            "None\n",
            "en-zh\n",
            "*openai_gpt-4o-mini_final_set_with_QA[noref]                      1  0.6172036  . = = > = > = > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step[noref]             1  0.6156714  . . = > = > = > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted[noref]    1  0.6139596  . . . > = > = > > > > > > >\n",
            "*m-pro[noref]                                                     2  0.6134360  . . . . = = = = = = = = = =\n",
            "*openai_gpt-4o-mini_final_set_with_QA_weighted[noref]             2  0.6113447  . . . . . > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2[noref]           3  0.6039912  . . . . . . = > = > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted[noref]  3  0.6038051  . . . . . . . > > > > > > >\n",
            "*gemba_all_orig[noref]                                            4  0.5907883  . . . . . . . . = = = = > >\n",
            "*cometkiwi[noref]                                                 4  0.5654377  . . . . . . . . . > > > > >\n",
            "*quar_fr_en_20250320_19440812300[noref]                           5  0.5379542  . . . . . . . . . . = > > >\n",
            "*comet_xl[noref]                                                  5  0.5370692  . . . . . . . . . . . > > >\n",
            "*comet_xxl[noref]                                                 6  0.5096220  . . . . . . . . . . . . > >\n",
            "*quar_xxen_20250320_21051212150[noref]                            7  0.4187092  . . . . . . . . . . . . . >\n",
            "*treqa[noref]                                                     8  0.3760059  . . . . . . . . . . . . . . \n",
            "None\n",
            "de-zh\n",
            "*openai_gpt-4o-mini_final_set_with_QA[noref]                      1  0.6036597  . = = > > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_QA_weighted[noref]             1  0.6003546  . . > > > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted[noref]    2  0.5911804  . . . > > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step[noref]             3  0.5856951  . . . . > = > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2[noref]           4  0.5764446  . . . . . = > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted[noref]  4  0.5754652  . . . . . . > > > > > > > >\n",
            "*m-pro[noref]                                                     5  0.5691587  . . . . . . . = = = = > > >\n",
            "*gemba_all_orig[noref]                                            5  0.5349783  . . . . . . . . > > > > > >\n",
            "*cometkiwi[noref]                                                 6  0.3665660  . . . . . . . . . > > > > >\n",
            "*quar_fr_en_20250320_19440812300[noref]                           7  0.2707199  . . . . . . . . . . > > > >\n",
            "*treqa[noref]                                                     8  0.2282566  . . . . . . . . . . . > > >\n",
            "*quar_xxen_20250320_21051212150[noref]                            9  0.1928486  . . . . . . . . . . . . > >\n",
            "*comet_xl[noref]                                                 10  0.1621538  . . . . . . . . . . . . . >\n",
            "*comet_xxl[noref]                                                11  0.1562549  . . . . . . . . . . . . . . \n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# @title Evaluate metrics using segment-level Kendall correlation\n",
        "\n",
        "# Kendall correlation is similar to pairwise accuracy, except that it is\n",
        "# normalized differently. The function calls are identical to the previous one,\n",
        "# except that we set the 'level' parameter to 'seg', and specify Kendall rather\n",
        "# than Pearson. The value of the 'average_by' parameter also matters here, as it\n",
        "# specifies how system x segment score matrices get converted into vectors for\n",
        "# comparison. We will use 'none', which just flattens the matrices.\n",
        "\n",
        "# The resulting ranking is similar to the ranking from accuracy. One noticeable\n",
        "# difference is that the significance clusters are smaller because they are\n",
        "# based on more data (much larger vectors). Notice that BLEU is absent because\n",
        "# it isn't available at the segment level.\n",
        "\n",
        "# need to fill na with some score\n",
        "dct = pd.DataFrame()\n",
        "\n",
        "for p in [\"en-de\", \"de-en\", \"en-zh\", \"de-zh\"]:\n",
        "    print(p)\n",
        "    evs = all_evs[\"liteval/\" + p]\n",
        "    corrs = data.GetCorrelations(\n",
        "        evs, 'seg', {evs.std_ref}, {'src'}, include_human, include_outliers,\n",
        "        gold_name, primary_metrics, domain)\n",
        "    ranks, matrix, _, _ = data.CompareMetrics(\n",
        "        corrs, stats.KendallVariants, 'none', k, psd, pval, perm_test = \"pairs\")\n",
        "\n",
        "    print(data.PrintMetricComparison(ranks, matrix, pval, evs))\n",
        "    tmp = pd.DataFrame(ranks).T[[0]]\n",
        "    tmp[\"pair\"] = p\n",
        "    tmp[\"metric\"] = \"segment-level_Kendall\"\n",
        "    m = pd.DataFrame(matrix)\n",
        "    m.columns = tmp.index\n",
        "    tmp = pd.concat([tmp.reset_index(drop = False), m.reset_index(drop = True)], axis = 1)\n",
        "    dct = pd.concat([dct, tmp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1P8jUG0kUEP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "en-de\n",
            "*openai_gpt-4o-mini_final_set_with_QA_weighted[noref]             1  0.6284585  . = = > > > > > > > > > > >\n",
            "*comet_xxl[noref]                                                 1  0.6248353  . . = = = > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted[noref]  1  0.6235178  . . . = > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_QA[noref]                      2  0.6212121  . . . . = > = > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted[noref]    2  0.6179183  . . . . . > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2[noref]           3  0.6175889  . . . . . . = = = = > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step[noref]             3  0.6149539  . . . . . . . > > > > > > >\n",
            "*comet_xl[noref]                                                  4  0.5757576  . . . . . . . . = = > > > >\n",
            "*cometkiwi[noref]                                                 4  0.5731225  . . . . . . . . . = = > > >\n",
            "*quar_fr_en_20250320_19440812300[noref]                           4  0.5701581  . . . . . . . . . . = > > >\n",
            "*quar_xxen_20250320_21051212150[noref]                            5  0.5635705  . . . . . . . . . . . > > >\n",
            "*gemba_all_orig[noref]                                            6  0.5546772  . . . . . . . . . . . . > >\n",
            "*m-pro[noref]                                                     7  0.4588274  . . . . . . . . . . . . . >\n",
            "*treqa[noref]                                                     8  0.4318182  . . . . . . . . . . . . . . \n",
            "None\n",
            "de-en\n",
            "*openai_gpt-4o-mini_final_set_with_QA_weighted[noref]             1  0.5589226  . > > > > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_QA[noref]                      2  0.5424242  . . > > > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted[noref]    3  0.5249158  . . . = = > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step[noref]             3  0.5215488  . . . . = > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted[noref]  3  0.5117845  . . . . . > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2[noref]           4  0.4986532  . . . . . . = > > > > > > >\n",
            "*comet_xxl[noref]                                                 4  0.4878788  . . . . . . . = > > > > > >\n",
            "*comet_xl[noref]                                                  5  0.4801347  . . . . . . . . = = > > > >\n",
            "*gemba_all_orig[noref]                                            5  0.4717172  . . . . . . . . . = = > > >\n",
            "*treqa[noref]                                                     5  0.4629630  . . . . . . . . . . = > > >\n",
            "*cometkiwi[noref]                                                 6  0.4589226  . . . . . . . . . . . > > >\n",
            "*quar_fr_en_20250320_19440812300[noref]                           7  0.4319865  . . . . . . . . . . . . = >\n",
            "*quar_xxen_20250320_21051212150[noref]                            7  0.4303030  . . . . . . . . . . . . . >\n",
            "*m-pro[noref]                                                     8  0.3521886  . . . . . . . . . . . . . . \n",
            "None\n",
            "en-zh\n",
            "*openai_gpt-4o-mini_final_set_with_QA_weighted[noref]             1  0.5896465  . = = > > = > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted[noref]    1  0.5855429  . . = = > = = > > > > > > >\n",
            "*quar_fr_en_20250320_19440812300[noref]                           1  0.5855429  . . . = = = = > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_QA[noref]                      2  0.5845960  . . . . = = = > > = > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step[noref]             2  0.5814394  . . . . . = = > = = > > > >\n",
            "*comet_xl[noref]                                                  2  0.5804924  . . . . . . = > = = > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted[noref]  2  0.5792298  . . . . . . . > > = > > > >\n",
            "*cometkiwi[noref]                                                 3  0.5744949  . . . . . . . . = = > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2[noref]           3  0.5719697  . . . . . . . . . = > > > >\n",
            "*comet_xxl[noref]                                                 3  0.5703914  . . . . . . . . . . > > > >\n",
            "*gemba_all_orig[noref]                                            4  0.5296717  . . . . . . . . . . . > > >\n",
            "*quar_xxen_20250320_21051212150[noref]                            5  0.4914773  . . . . . . . . . . . . > >\n",
            "*m-pro[noref]                                                     6  0.4640152  . . . . . . . . . . . . . =\n",
            "*treqa[noref]                                                     6  0.4548611  . . . . . . . . . . . . . . \n",
            "None\n",
            "de-zh\n",
            "*openai_gpt-4o-mini_final_set_with_QA_weighted[noref]             1  0.6856061  . > > > > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_QA[noref]                      2  0.6773416  . . = > > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted[noref]    2  0.6707989  . . . > > > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_Qlevel_step[noref]             3  0.6635675  . . . . = = > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted[noref]  3  0.6608127  . . . . . > > > > > > > > >\n",
            "*openai_gpt-4o-mini_final_set_with_plevel_stepv2[noref]           4  0.6525482  . . . . . . > > > > > > > >\n",
            "*cometkiwi[noref]                                                 5  0.6026171  . . . . . . . > > > > > > >\n",
            "*gemba_all_orig[noref]                                            6  0.5816116  . . . . . . . . = > > > > >\n",
            "*quar_fr_en_20250320_19440812300[noref]                           6  0.5781680  . . . . . . . . . > > > > >\n",
            "*treqa[noref]                                                     7  0.5251377  . . . . . . . . . . > > > >\n",
            "*m-pro[noref]                                                     8  0.5044766  . . . . . . . . . . . = > >\n",
            "*quar_xxen_20250320_21051212150[noref]                            8  0.4879477  . . . . . . . . . . . . = =\n",
            "*comet_xxl[noref]                                                 9  0.4779614  . . . . . . . . . . . . . =\n",
            "*comet_xl[noref]                                                  9  0.4755510  . . . . . . . . . . . . . . \n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# @title Evaluate metrics using seg-level accuracy with optimized tie threshold.\n",
        "\n",
        "# This is an implementation of the acc*_eq pairwise ranking accuracy proposed in\n",
        "# https://arxiv.org/abs/2305.14324. This is similar to global accuracy, but it\n",
        "# additionally gives metrics credit for predicting ties in gold scores, which\n",
        "# arise frequently in MQM segment-level data. To avoid bias due to differences\n",
        "# in scoring precision for different metrics, an optimal threshold for assigning\n",
        "# ties is automatically computed for each metric and test set.\n",
        "\n",
        "# For demo purposes we disable significance testing by setting k to 0.\n",
        "# (Significance testing works but is currently very slow.) Note that the\n",
        "# optimization procedure uses sampling, so results can change across different\n",
        "# runs.\n",
        "\n",
        "\n",
        "for p in [\"en-de\", \"de-en\", \"en-zh\", \"de-zh\"]:\n",
        "    print(p)\n",
        "    evs = all_evs[\"liteval/\" + p]\n",
        "    corrs = data.GetCorrelations(\n",
        "        evs, 'seg', {evs.std_ref}, {'src'}, include_human, include_outliers,\n",
        "        gold_name, primary_metrics, domain)\n",
        "    ranks, matrix, _, _ = data.CompareMetrics(\n",
        "        corrs, stats.KendallWithTiesOpt, 'item', k, psd, pval, variant='acc23',\n",
        "        sample_rate=1, perm_test = \"pairs\")\n",
        "\n",
        "    print(data.PrintMetricComparison(ranks, matrix, pval, evs))\n",
        "    tmp = pd.DataFrame(ranks).T[[0]]\n",
        "    tmp[\"pair\"] = p\n",
        "    tmp[\"metric\"] = \"acc_eq\"\n",
        "    m = pd.DataFrame(matrix)\n",
        "    m.columns = tmp.index\n",
        "    tmp = pd.concat([tmp.reset_index(drop = False), m.reset_index(drop = True)], axis = 1)\n",
        "    dct = pd.concat([dct, tmp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dct.to_csv(\"significance_test.csv\", index=False)\n",
        "dct.drop(columns = [\"level_0\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "#dct.reset_index(inplace=True, drop = True)\n",
        "dct.columns = [\"metric\", \"score\", \"pair\", \"corr\", 'openai_gpt-4o-mini_final_set_with_QA_weighted-src',\n",
        "                           'openai_gpt-4o-mini_final_set_with_QA-src',\n",
        "         'openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted-src',\n",
        "       'openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted-src',\n",
        "                  'openai_gpt-4o-mini_final_set_with_Qlevel_step-src',\n",
        "                'openai_gpt-4o-mini_final_set_with_plevel_stepv2-src',\n",
        "                                                          'm-pro-src',\n",
        "                                                 'gemba_all_orig-src',\n",
        "                                                      'comet_xxl-src',\n",
        "                                                      'cometkiwi-src',\n",
        "                                                       'comet_xl-src',\n",
        "                                'quar_fr_en_20250320_19440812300-src',\n",
        "                                 'quar_xxen_20250320_21051212150-src',\n",
        "                                                          'treqa-src']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>corr</th>\n",
              "      <th>acc_eq</th>\n",
              "      <th>segment-level_Kendall</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>comet_xl-src</th>\n",
              "      <td>0.527984</td>\n",
              "      <td>0.386816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comet_xxl-src</th>\n",
              "      <td>0.540267</td>\n",
              "      <td>0.399597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cometkiwi-src</th>\n",
              "      <td>0.552289</td>\n",
              "      <td>0.455079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gemba_all_orig-src</th>\n",
              "      <td>0.534419</td>\n",
              "      <td>0.560933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>m-pro-src</th>\n",
              "      <td>0.444877</td>\n",
              "      <td>0.569929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>openai_gpt-4o-mini_final_set_with_QA-src</th>\n",
              "      <td>0.606393</td>\n",
              "      <td>0.605087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>openai_gpt-4o-mini_final_set_with_QA_weighted-src</th>\n",
              "      <td>0.615658</td>\n",
              "      <td>0.604629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>openai_gpt-4o-mini_final_set_with_Qlevel_step-src</th>\n",
              "      <td>0.595377</td>\n",
              "      <td>0.594319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted-src</th>\n",
              "      <td>0.599794</td>\n",
              "      <td>0.596584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>openai_gpt-4o-mini_final_set_with_plevel_stepv2-src</th>\n",
              "      <td>0.585190</td>\n",
              "      <td>0.585067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted-src</th>\n",
              "      <td>0.593836</td>\n",
              "      <td>0.587145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quar_fr_en_20250320_19440812300-src</th>\n",
              "      <td>0.541464</td>\n",
              "      <td>0.406096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quar_xxen_20250320_21051212150-src</th>\n",
              "      <td>0.493325</td>\n",
              "      <td>0.348311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>treqa-src</th>\n",
              "      <td>0.468695</td>\n",
              "      <td>0.314291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "corr                                                  acc_eq  \\\n",
              "metric                                                         \n",
              "comet_xl-src                                        0.527984   \n",
              "comet_xxl-src                                       0.540267   \n",
              "cometkiwi-src                                       0.552289   \n",
              "gemba_all_orig-src                                  0.534419   \n",
              "m-pro-src                                           0.444877   \n",
              "openai_gpt-4o-mini_final_set_with_QA-src            0.606393   \n",
              "openai_gpt-4o-mini_final_set_with_QA_weighted-src   0.615658   \n",
              "openai_gpt-4o-mini_final_set_with_Qlevel_step-src   0.595377   \n",
              "openai_gpt-4o-mini_final_set_with_Qlevel_step_w...  0.599794   \n",
              "openai_gpt-4o-mini_final_set_with_plevel_stepv2...  0.585190   \n",
              "openai_gpt-4o-mini_final_set_with_plevel_stepv2...  0.593836   \n",
              "quar_fr_en_20250320_19440812300-src                 0.541464   \n",
              "quar_xxen_20250320_21051212150-src                  0.493325   \n",
              "treqa-src                                           0.468695   \n",
              "\n",
              "corr                                                segment-level_Kendall  \n",
              "metric                                                                     \n",
              "comet_xl-src                                                     0.386816  \n",
              "comet_xxl-src                                                    0.399597  \n",
              "cometkiwi-src                                                    0.455079  \n",
              "gemba_all_orig-src                                               0.560933  \n",
              "m-pro-src                                                        0.569929  \n",
              "openai_gpt-4o-mini_final_set_with_QA-src                         0.605087  \n",
              "openai_gpt-4o-mini_final_set_with_QA_weighted-src                0.604629  \n",
              "openai_gpt-4o-mini_final_set_with_Qlevel_step-src                0.594319  \n",
              "openai_gpt-4o-mini_final_set_with_Qlevel_step_w...               0.596584  \n",
              "openai_gpt-4o-mini_final_set_with_plevel_stepv2...               0.585067  \n",
              "openai_gpt-4o-mini_final_set_with_plevel_stepv2...               0.587145  \n",
              "quar_fr_en_20250320_19440812300-src                              0.406096  \n",
              "quar_xxen_20250320_21051212150-src                               0.348311  \n",
              "treqa-src                                                        0.314291  "
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t1 = dct.groupby([\"corr\", \"metric\"], as_index=False)[\"score\"].mean()\n",
        "# pivot table \n",
        "t1.pivot(index = \"metric\", columns = \"corr\", values = \"score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if human scores are ranked higher than system scores for each source\n",
        "def check_humanvsllm(df_list, metric, source_col = \"source_\", human_col = [\"translator1\", \"translator2\", \"translator3\"], llm_col = [\"gpt4o\", \"google_translate\", \"qwen2\", \"deepl\"]):\n",
        "    lst = []\n",
        "    for p in df_list[\"pair\"].unique():\n",
        "        n_humanllm = 0\n",
        "        n_humanmt = 0\n",
        "        n_smt = 0\n",
        "        count = 0\n",
        "        for source_, g in df_list[df_list[\"pair\"] == p].groupby([source_col]):\n",
        "            g.drop_duplicates(subset=[\"model\", source_col], inplace=True)\n",
        "            human_m = set(g[\"model\"]).intersection(set(human_col))\n",
        "            mt_m = set(g[\"model\"]).difference(human_m)\n",
        "            smt_m = set(g[\"model\"]).difference(human_m).difference(set(llm_col))\n",
        "            human_max = g.set_index([\"model\"]).loc[list(human_m), metric].max()\n",
        "            mt_max = g.set_index([\"model\"]).loc[list(mt_m), metric].max()\n",
        "            if human_max > mt_max:\n",
        "                n_humanmt += 1\n",
        "            if llm_col:\n",
        "                llm_m = set(g[\"model\"]).intersection(set(llm_col))\n",
        "                llm_max = g.set_index([\"model\"]).loc[list(llm_m), metric].max()\n",
        "                smt_max = g.set_index([\"model\"]).loc[list(smt_m), metric].max()\n",
        "                if human_max > llm_max:\n",
        "                    n_humanllm += 1\n",
        "                if human_max > smt_max:\n",
        "                    n_smt += 1\n",
        "            count += 1\n",
        "        if count > 0:\n",
        "            lst.append((p, metric, n_humanllm/count, n_humanmt/count, n_smt/count))\n",
        "        else:\n",
        "            print(source_, g.model.unique())\n",
        "            print(p, metric, \"no human scores\")\n",
        "            break\n",
        "    return lst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mqm_score\n",
            "gemba_all_orig\n",
            "comet_xl\n",
            "comet_xxl\n",
            "m-pro\n",
            "cometkiwi\n",
            "treqa\n",
            "openai_gpt-4o-mini_final_set_with_QA\n",
            "openai_gpt-4o-mini_final_set_with_QA_weighted\n",
            "openai_gpt-4o-mini_final_set_with_plevel_stepv2\n",
            "openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted\n",
            "openai_gpt-4o-mini_final_set_with_Qlevel_step\n",
            "openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted\n",
            "quar_fr_en_20250320_19440812300\n",
            "quar_xxen_20250320_21051212150\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "adequacy_df = pd.DataFrame()\n",
        "for m in metrics:\n",
        "    print(m)\n",
        "    tmp = pd.DataFrame(check_humanvsllm(df_list, m), columns=[\"pair\", \"metric\", \"humanllm\", \"humanmt\", \"humansmallmt\"])\n",
        "    adequacy_df = pd.concat([adequacy_df, tmp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>metric</th>\n",
              "      <th>humanllm</th>\n",
              "      <th>humanmt</th>\n",
              "      <th>humansmallmt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>comet_xl</td>\n",
              "      <td>0.170256</td>\n",
              "      <td>0.119619</td>\n",
              "      <td>0.544845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>comet_xxl</td>\n",
              "      <td>0.266853</td>\n",
              "      <td>0.239065</td>\n",
              "      <td>0.611760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cometkiwi</td>\n",
              "      <td>0.073490</td>\n",
              "      <td>0.062253</td>\n",
              "      <td>0.525899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gemba_all_orig</td>\n",
              "      <td>0.061027</td>\n",
              "      <td>0.061027</td>\n",
              "      <td>0.631240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>m-pro</td>\n",
              "      <td>0.164847</td>\n",
              "      <td>0.148054</td>\n",
              "      <td>0.566608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>mqm_score</td>\n",
              "      <td>0.452761</td>\n",
              "      <td>0.436215</td>\n",
              "      <td>0.868286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>openai_gpt-4o-mini_final_set_with_QA</td>\n",
              "      <td>0.387029</td>\n",
              "      <td>0.370483</td>\n",
              "      <td>0.856596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>openai_gpt-4o-mini_final_set_with_QA_weighted</td>\n",
              "      <td>0.414118</td>\n",
              "      <td>0.403128</td>\n",
              "      <td>0.856717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>openai_gpt-4o-mini_final_set_with_Qlevel_step</td>\n",
              "      <td>0.258561</td>\n",
              "      <td>0.225349</td>\n",
              "      <td>0.801377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted</td>\n",
              "      <td>0.269925</td>\n",
              "      <td>0.236712</td>\n",
              "      <td>0.807058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>openai_gpt-4o-mini_final_set_with_plevel_stepv2</td>\n",
              "      <td>0.319499</td>\n",
              "      <td>0.297145</td>\n",
              "      <td>0.823236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted</td>\n",
              "      <td>0.363266</td>\n",
              "      <td>0.340912</td>\n",
              "      <td>0.839803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>quar_fr_en_20250320_19440812300</td>\n",
              "      <td>0.103420</td>\n",
              "      <td>0.080945</td>\n",
              "      <td>0.492061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>quar_xxen_20250320_21051212150</td>\n",
              "      <td>0.233767</td>\n",
              "      <td>0.211413</td>\n",
              "      <td>0.423353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>treqa</td>\n",
              "      <td>0.119619</td>\n",
              "      <td>0.066015</td>\n",
              "      <td>0.220192</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      metric  humanllm  \\\n",
              "0                                                   comet_xl  0.170256   \n",
              "1                                                  comet_xxl  0.266853   \n",
              "2                                                  cometkiwi  0.073490   \n",
              "3                                             gemba_all_orig  0.061027   \n",
              "4                                                      m-pro  0.164847   \n",
              "5                                                  mqm_score  0.452761   \n",
              "6                       openai_gpt-4o-mini_final_set_with_QA  0.387029   \n",
              "7              openai_gpt-4o-mini_final_set_with_QA_weighted  0.414118   \n",
              "8              openai_gpt-4o-mini_final_set_with_Qlevel_step  0.258561   \n",
              "9     openai_gpt-4o-mini_final_set_with_Qlevel_step_weighted  0.269925   \n",
              "10           openai_gpt-4o-mini_final_set_with_plevel_stepv2  0.319499   \n",
              "11  openai_gpt-4o-mini_final_set_with_plevel_stepv2_weighted  0.363266   \n",
              "12                           quar_fr_en_20250320_19440812300  0.103420   \n",
              "13                            quar_xxen_20250320_21051212150  0.233767   \n",
              "14                                                     treqa  0.119619   \n",
              "\n",
              "     humanmt  humansmallmt  \n",
              "0   0.119619      0.544845  \n",
              "1   0.239065      0.611760  \n",
              "2   0.062253      0.525899  \n",
              "3   0.061027      0.631240  \n",
              "4   0.148054      0.566608  \n",
              "5   0.436215      0.868286  \n",
              "6   0.370483      0.856596  \n",
              "7   0.403128      0.856717  \n",
              "8   0.225349      0.801377  \n",
              "9   0.236712      0.807058  \n",
              "10  0.297145      0.823236  \n",
              "11  0.340912      0.839803  \n",
              "12  0.080945      0.492061  \n",
              "13  0.211413      0.423353  \n",
              "14  0.066015      0.220192  "
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "adequacy_df.groupby([\"metric\"], as_index=False)[[\"humanllm\", \"humanmt\", \"humansmallmt\"]].mean()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "mt_metrics_eval.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1gXA-HQKMF6G4IdrUob8hPnbVm6_rsfeX",
          "timestamp": 1656987947120
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "metric",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
